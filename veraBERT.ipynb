{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f02e8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer,get_linear_schedule_with_warmup,RobertaTokenizer,BertForSequenceClassification,BitsAndBytesConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm.notebook import tqdm\n",
    "import psutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import get_peft_model\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a581e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecurityBERT(nn.Module):\n",
    "  def __init__(self,myTunedBERT,n_classes):\n",
    "    super(SecurityBERT,self).__init__()\n",
    "    self.bert = myTunedBERT\n",
    "    self.dropout = nn.Dropout(p=0.1)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size,n_classes)\n",
    "    self.config = self.bert.config\n",
    "    self.gradient_checkpointing_enable = self.bert.gradient_checkpointing_enable\n",
    "\n",
    "  def forward(self,input_ids,attention_mask):\n",
    "    pooled_output = self.bert(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    ).pooler_output\n",
    "\n",
    "    output = self.dropout(pooled_output)\n",
    "\n",
    "    return self.out(output)\n",
    "  \n",
    "def print_trainable_parameters(model):\n",
    "    if isinstance(model,SecurityBERT):\n",
    "        trainable = sum(p.numel() for p in model.bert.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable:,}\")\n",
    "    print(f\"Total parameters: {total:,}\")\n",
    "    print(f\"Percentage of trainable params: {100 * trainable / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b6500b",
   "metadata": {},
   "source": [
    "## Set up model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7adfbe8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a9c1b1f92e4ebf9b6e265bd03e753e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/528 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e32f3a33232433790adb55994edfcf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/17.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at gaunernst/bert-tiny-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Tokenizer length: 30522\n",
      "Trainable parameters: 4,386,178\n",
      "Total parameters: 4,386,178\n",
      "Percentage of trainable params: 100.00%\n",
      "Number of available GPUs: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gaunernst/bert-tiny-uncased\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "print(model.config)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('./tokenizer')\n",
    "print(f\"Tokenizer length: {len(tokenizer.get_vocab())}\")\n",
    "model.resize_token_embeddings(len(tokenizer)) # Resize the vocabulary without invalidating pre-trained weights\n",
    "print_trainable_parameters(model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "886df213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_check():\n",
    "  return round(psutil.virtual_memory().used/1024**3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee3a926",
   "metadata": {},
   "source": [
    "## PEFT Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccb89eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 544\n",
      "Total parameters: 4,386,722\n",
      "Percentage of trainable params: 0.01%\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, VeraConfig, TaskType\n",
    "\n",
    "# Define VeRA configuration\n",
    "config = VeraConfig(\n",
    "    r=8,  # Low-rank decomposition size\n",
    "    target_modules=[\"query\", \"value\"],  # Specify target modules\n",
    "    vera_dropout=0.2,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Apply VeRA adapter\n",
    "#model = model.prepare_model_for_kbit_training()\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75b15a8",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75ba8fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.read_pickle('./saved_data/encoded_data.pck')\n",
    "le = LabelEncoder()\n",
    "data['target'] = le.fit_transform(data['Attack_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "650b3a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded_PPFLE</th>\n",
       "      <th>Attack_type</th>\n",
       "      <th>Attack_label</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80984</th>\n",
       "      <td>1091d7634bb290943eddba5b408ce4d81fcda2a2 0bc16...</td>\n",
       "      <td>Vulnerability_scanner</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50499</th>\n",
       "      <td>a0801f4dbf48dae5beb48d0458f4dc7e2d8fb7d5 0bc16...</td>\n",
       "      <td>DDoS_TCP</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72969</th>\n",
       "      <td>8582326c3f4225dfc21cab8323295eb6c8c82646 6df8d...</td>\n",
       "      <td>Port_Scanning</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           encoded_PPFLE  \\\n",
       "80984  1091d7634bb290943eddba5b408ce4d81fcda2a2 0bc16...   \n",
       "50499  a0801f4dbf48dae5beb48d0458f4dc7e2d8fb7d5 0bc16...   \n",
       "72969  8582326c3f4225dfc21cab8323295eb6c8c82646 6df8d...   \n",
       "\n",
       "                 Attack_type  Attack_label  target  \n",
       "80984  Vulnerability_scanner             1      13  \n",
       "50499               DDoS_TCP             1       3  \n",
       "72969          Port_Scanning             1       9  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d3e00cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'689e1d00f39f485bd2da5098239199df85c39b4d 2ab932d3aadb7887eda7302e0b33df13dc5ca645 f2c86566785eaad29f5c4b244d058b0cb7deb97e 5ea155b76662c1b381a0526e87d733a2ae68bfce b3625a98258ceacc69889389baca3b048a4923e4 8d419a4b06c3bd95d00fe2ee5cf1146851cf9893 c71f6d1e2daf2f598381088226df35ae68552f0b b7036347f54fa5aef0dd3f66b4e44c56bbf54cf4 f97d947902327761a4b1472d76b71bf4406e4e18 79ae7b95cace51fb0079d9c5ef1641f4639ce3ed ea25a11eb9c83b7d70252710098ded6bc81eb62b ceae604760aaa4fd8b60791ae1839b754c2dd9f1 23197a0ba8a217f2bc8711ec3c4be39dc19236d7 e2df55df19acd4423bafccbb0b8a385ec5e914eb f086df09fafdda401160bc766631e3d5785a6257 5329d5d159837e103352035068b3e14ce570d7eb 3eb6e4b9dd1c085a3ad0b18484d48482c935a4f7 943315f7b6cb0fc45604f5cf2ac307894e59b990 89b029338e8dd73f49369cd01c0052663c8c949e f91e2919e7494d796d1148066124d4b4939c86fa a5ce077c8900f85e073032ae975d95491645ddce effcfb2aa80217f507218ea88726c9325a0d68b9 bf12d74b6502b4fc1e29df164d3bf6f6ce51ffe3 7d69c36dbc4a37fc6643a32e914ea4e7ee0f0fc2 128f4c3fdc8539ea887fc54fe41fb2654e61d21e cf09211f8fce92ebb8209ea5703c84c95adababf 527ff8dab3e2a9038784c4498898f385d37c8bbf 25228e41a07fb6337440bcd65bf610730baee1ad 2bb8fbd462d958539a98c45da70be02731b9c83d e08c348b0a504bcd8779123ba7fca91b068ab03c e3773f9e7df4f1e1ac4a50cc8c7ad74dcfa5caea afb3c4e997ed979faa4be5b9acdfa907baf289bc 8c013b2163fe03dbadcbebe166b14f5eaa7b696b 609efeabea8e6b3209a269279f8981c91e174f55 9d8f0cbe88bc5a9c776d85c62f93520e64171b87 ac85af4c5313ae22e35f29e0ad4cc7d859d4b73a 471a1d5c286644db75306d1d9c9cbe5249f4e98e 10ded1179d3349c7594e5117de982d951d4de1fc 1aea5dc09556ef08f620c7b8d3a816e9a812e9c3 321d843de46a66adf2f508493b803187b9f3a488 2dadbb6a5bb73f1bc9a604a27ed3d9e47ab23299 20bf31a2b3be346947648c9c27876bc5f8b50de4 0e24afc14051165f920027d89f9f2185037c3654 15ba9de63999d8f46389024fc2f1c6ed7a3e51f1 fafed74d6fbd09b6f066aa83ba4a38e30e4379c5 094658d5fc003bad5c085219ff65122ddc885f1d 836658e7678c4b7410183ed50b0e263ebd6665bf df02b7c8b68f6ec003c80d1c814f037dfcd55b9f 14520d20adff40fb27695aae63868abe56dcaf25 d4ec9f131ac9b992101f89975d1aa9525f50faac c670faa4b0c2f337be3a5dd7023896927af82abb 5d4aa9cc3a38425b4861a39bf306f278c34c5915 33ff04f381603ec87b266a7f7d8ee63230d3ab4d 6c688e57ffaaa3e0f6b832b1698a0e0b4e166df9 fd64ac0598fbeb73ed97cf30b69998dcc1c7a55c 7ff6f594e600ece857707fa8b01534124647de42 7f7f4bd794be0a8e3ad16af50933b3725c30443a 1b3388c83b9ea78221c264bc4bd2b9ed163df9c5 4d2fc9148ddb9459165087751a8d0b6bb846681a afc155dfb40593981720bbd50f68eaf71891b9dd 4eb7e229a7ed9d2c82281d958731e5237b422651'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['encoded_PPFLE'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "428603e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(data['target'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9154124f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.934"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9ea4770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41d8100b5314516b50e7ebf88ecef65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"encoded_data.txt\",\"w\") as f:\n",
    "  for value in tqdm(data['encoded_PPFLE']):\n",
    "    f.write(str(value)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04e43284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110460, 4) (23670, 4) (23670, 4)\n"
     ]
    }
   ],
   "source": [
    "train_set = data.sample(frac=0.7,random_state=42).reset_index(drop=True)\n",
    "\n",
    "remaining = data.drop(train_set.index).reset_index(drop=True)\n",
    "\n",
    "test_set = remaining.sample(frac=0.5,random_state=42).reset_index(drop=True)\n",
    "\n",
    "val_set = remaining.drop(test_set.index).reset_index(drop=True)\n",
    "\n",
    "print(train_set.shape,val_set.shape,test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8702316",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=test_ratio,stratify=data.iloc[:,-1], random_state=42)\n",
    "train_set, val_set = train_test_split(train_set, test_size=val_ratio/(val_ratio+train_ratio),stratify=train_set.iloc[:,-1], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c34b9833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((110460, 4), (23670, 4), (23670, 4))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape,val_set.shape,test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bfca417",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LIST = ['Backdoor', 'DDoS_HTTP', 'DDoS_ICMP', 'DDoS_TCP', 'DDoS_UDP',\n",
    "                'Fingerprinting', 'MITM', 'Normal', 'Password', 'Port_Scanning',\n",
    "                'Ransomware', 'SQL_injection', 'Uploading', 'Vulnerability_scanner',\n",
    "                'XSS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bf22757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded_PPFLE</th>\n",
       "      <th>Attack_type</th>\n",
       "      <th>Attack_label</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13140</th>\n",
       "      <td>05293357ba325a590027981a9b59eb748a968bb8 0bc16...</td>\n",
       "      <td>Uploading</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13141</th>\n",
       "      <td>da0992898db14d190c3b6f7c2c6c658259a2b3a4 0bc16...</td>\n",
       "      <td>Uploading</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13142</th>\n",
       "      <td>4d5b89f0a6797f41a5d0ee5b7b1cfe1327a291f4 0bc16...</td>\n",
       "      <td>Uploading</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13143</th>\n",
       "      <td>732fcefab7fb269fd8ae7bd4d3b3e28ec3310e1f 6df8d...</td>\n",
       "      <td>Uploading</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13144</th>\n",
       "      <td>9a7d459b7d8a0e5c070cb85c3b1bd0824a7b010f 0bc16...</td>\n",
       "      <td>Uploading</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           encoded_PPFLE Attack_type  \\\n",
       "13140  05293357ba325a590027981a9b59eb748a968bb8 0bc16...   Uploading   \n",
       "13141  da0992898db14d190c3b6f7c2c6c658259a2b3a4 0bc16...   Uploading   \n",
       "13142  4d5b89f0a6797f41a5d0ee5b7b1cfe1327a291f4 0bc16...   Uploading   \n",
       "13143  732fcefab7fb269fd8ae7bd4d3b3e28ec3310e1f 6df8d...   Uploading   \n",
       "13144  9a7d459b7d8a0e5c070cb85c3b1bd0824a7b010f 0bc16...   Uploading   \n",
       "\n",
       "       Attack_label  target  \n",
       "13140             1      12  \n",
       "13141             1      12  \n",
       "13142             1      12  \n",
       "13143             1      12  \n",
       "13144             1      12  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['Attack_type']=='Uploading'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d5194fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self,df,tokenizer,max_len):\n",
    "    self.df = df\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len=max_len\n",
    "    self.sequence = self.df['encoded_PPFLE'].tolist()\n",
    "    self.targets = self.df['target'].tolist()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    sequence = str(self.sequence[idx])\n",
    "    target = self.targets[idx]\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "        sequence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'input_ids':encoding['input_ids'].flatten(),\n",
    "        'attention_mask':encoding['attention_mask'].flatten(),\n",
    "        'targets':torch.tensor(target,dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6dbc19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=512\n",
    "BATCH_SIZE=32\n",
    "\n",
    "train_dataset = CustomDataset(train_set,tokenizer=tokenizer,max_len=MAX_LEN)\n",
    "val_dataset = CustomDataset(val_set,tokenizer=tokenizer,max_len=MAX_LEN)\n",
    "test_dataset = CustomDataset(test_set,tokenizer=tokenizer,max_len=MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0\n",
    "\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0\n",
    "\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f83d27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "test_data = next(iter(train_loader))\n",
    "\n",
    "print(test_data['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37ece3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chkpt(model,version):\n",
    "  return model.load_state_dict(torch.load(f\"./saved_model/securityBert{version}.0.pt\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d8d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_model_peft(trainer,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n",
    "  trainer.train()\n",
    "\n",
    "  print(\"after\")\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  for d in data_loader:\n",
    "    input_ids = d['input_ids'].to(device)\n",
    "    attention_mask = d['attention_mask'].to(device)\n",
    "    targets = d['targets'].to(device)\n",
    "    print(\"the\")\n",
    "\n",
    "    outputs = trainer.model(input_ids,attention_mask)\n",
    "    _,preds = torch.max(outputs,dim=1)\n",
    "    loss = loss_fn(outputs,targets)\n",
    "\n",
    "    correct_predictions+=torch.sum(preds==targets).cpu()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(trainer.model.parameters(),max_norm=1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions/n_examples,np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e619e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_model_peft(trainer,data_loader,loss_fn,device,n_examples):\n",
    "  trainer.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  for d in data_loader:\n",
    "    input_ids = d['input_ids'].to(device)\n",
    "    attention_mask = d['attention_mask'].to(device)\n",
    "    targets = d['targets'].to(device)\n",
    "\n",
    "    outputs = trainer.model(input_ids,attention_mask)\n",
    "    _,preds = torch.max(outputs,dim=1)\n",
    "\n",
    "    loss = loss_fn(outputs,targets)\n",
    "\n",
    "    correct_predictions+=torch.sum(preds==targets).cpu()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions/n_examples,np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19453ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecurityBERT(nn.Module):\n",
    "  def __init__(self,myTunedBERT,n_classes):\n",
    "    super(SecurityBERT,self).__init__()\n",
    "    self.bert = myTunedBERT\n",
    "    self.dropout = nn.Dropout(p=0.1)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size,n_classes)\n",
    "    self.config = self.bert.config\n",
    "    self.gradient_checkpointing_enable = self.bert.gradient_checkpointing_enable\n",
    "\n",
    "  def forward(self,input_ids,attention_mask):\n",
    "    pooled_output = self.bert(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    ).pooler_output\n",
    "\n",
    "    output = self.dropout(pooled_output)\n",
    "\n",
    "    return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b23ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "securityBertTinyVera = SecurityBERT(myTunedBERT=model,n_classes=len(TARGET_LIST)).to(device)\n",
    "EPOCHS=3\n",
    "optimizer_vera = torch.optim.AdamW(securityBertTinyVera.parameters(),lr=1e-5)\n",
    "total_steps = len(train_loader)*EPOCHS\n",
    "\n",
    "scheduler_vera = get_linear_schedule_with_warmup(\n",
    "    optimizer_vera,\n",
    "    num_warmup_steps= 0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5333462d",
   "metadata": {},
   "source": [
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28d5697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# output dir \n",
    "model_version = \"securityBert_TinyVeRA_\"\n",
    "model_dir = f\"{model_version}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    run_name=model_version,\n",
    "    output_dir=model_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=1,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    logging_dir=f\"{model_dir}/logs\",\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    dataloader_num_workers=4,  # Adjust based on your CPU capabilities\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing to save memory\n",
    "    report_to=\"none\"  # Disable reporting to avoid unnecessary overhead\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1362e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potentially remove\n",
    "\n",
    "#securityBertTinyVera.bert.gradient_checkpointing_enable()\n",
    "#securityBertTinyVera = prepare_model_for_kbit_training(securityBertTinyVera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23553dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 544\n",
      "Total parameters: 4,386,722\n",
      "Percentage of trainable params: 0.01%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, confusion_matrix\n",
    "# The parameters after appling LoRA\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# designing computing metrics as per our use case. (F1-Macro is essential and log-loss is optional)\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p.predictions, TARGET_LIST\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(TARGET_LIST, predictions)\n",
    "    macro_f1 = f1_score(TARGET_LIST, predictions, average='macro')\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"macro_f1\": macro_f1}\n",
    "\n",
    "# configure Trainer\n",
    "trainer_vera = Trainer(\n",
    "    model=securityBertTinyVera,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2fa6d2",
   "metadata": {},
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from collections import defaultdict\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c96e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bdd646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deeabb15840941f6b744aada1141c102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history_tiny_vera = defaultdict(list)\n",
    "best_accuracy_tiny_vera=0\n",
    "print(\"1\")\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "  print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "  train_acc_tiny_vera,train_loss_tiny_vera = train_model_peft(trainer_vera,train_loader,loss_fn,optimizer_vera,device,scheduler_vera,len(train_set))\n",
    "  val_acc_tiny_vera,val_loss_tiny_vera = evaluation_model_peft(trainer_vera,val_loader,loss_fn,device,len(val_set))\n",
    "  history_tiny_vera['train_acc'].append(train_acc_tiny_vera)\n",
    "  history_tiny_vera['train_loss'].append(train_loss_tiny_vera)\n",
    "  history_tiny_vera['val_acc'].append(val_acc_tiny_vera)\n",
    "  history_tiny_vera['val_loss'].append(val_loss_tiny_vera)\n",
    "  print(f\"Train Loss {train_loss_tiny_vera} | Validation Loss {val_loss_tiny_vera} | Training Accuracy {train_acc_tiny_vera} | Validation Accuracy {val_acc_tiny_vera}\")\n",
    "\n",
    "  if val_acc_tiny_vera>best_accuracy_tiny_vera:\n",
    "    trainer.save_model(f\"./saved_model/{model_version}{epoch+1}.0.pt\")\n",
    "    best_accuracy_tiny_vera = val_acc_tiny_vera\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "history_tiny_vera['training_time'].append(end_time - start_time)\n",
    "\n",
    "# Convert to regular dict and save as JSON\n",
    "with open(\"./saved_model/history_tiny_vera.txt\", \"w\") as f:\n",
    "    json.dump(history_tiny_vera, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a099528c",
   "metadata": {},
   "source": [
    "Stopped trying to tune BERT-mini-uncased pre-trained, using QDoRA at 987 minutes. Did not get past first epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df6c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON and convert back to defaultdict\n",
    "with open(\"./saved_model/history_tiny_vera.txt\", \"r\") as f:\n",
    "    history_tiny_vera_json = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
