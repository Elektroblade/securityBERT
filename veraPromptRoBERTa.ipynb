{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13462dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer,get_linear_schedule_with_warmup,RobertaTokenizer,BertForSequenceClassification,BitsAndBytesConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm.notebook import tqdm\n",
    "import psutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import get_peft_model\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e004ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable:,}\")\n",
    "    print(f\"Total parameters: {total:,}\")\n",
    "    print(f\"Percentage of trainable params: {100 * trainable / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a80d5",
   "metadata": {},
   "source": [
    "## Custom RoBERTa class needs work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b8a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "class RobertaForSequenceClassificationWithCustomHead(RobertaForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Add a new custom layer, e.g., a small MLP after the original classifier\n",
    "        self.custom_layer = nn.Sequential(\n",
    "            nn.Linear(config.num_labels, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, config.num_labels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "\n",
    "        # Remove any unexpected keys that RobertaForSequenceClassification won't accept\n",
    "        accepted_keys = {\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\", \"position_ids\", \"head_mask\", \"inputs_embeds\", \"output_attentions\", \"output_hidden_states\", \"return_dict\"}\n",
    "        filtered_kwargs = {k: v for k, v in kwargs.items() if k in accepted_keys}\n",
    "\n",
    "        # Safe call to parent forward\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            **filtered_kwargs\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits\n",
    "        custom_logits = self.custom_layer(logits)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(custom_logits, labels)\n",
    "\n",
    "        return type(outputs)(\n",
    "            loss=loss,\n",
    "            logits=custom_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b4f4d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassificationWithCustomHead were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'custom_layer.0.bias', 'custom_layer.0.weight', 'custom_layer.2.bias', 'custom_layer.2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Trainable parameters: 124,659,166\n",
      "Total parameters: 124,659,166\n",
      "Percentage of trainable params: 100.00%\n",
      "Number of available GPUs: 1\n",
      "GPU Name: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "model = RobertaForSequenceClassificationWithCustomHead.from_pretrained(\"roberta-base\", num_labels=15)\n",
    "print(model.config)\n",
    "print_trainable_parameters(model)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "device\n",
    "\n",
    "# Pretrained RoBERTa tokenizer (e.g. roberta-base)\n",
    "pretrained_tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Your custom tokenizer trained on OOL tokens\n",
    "custom_tokenizer = RobertaTokenizerFast.from_pretrained('./tokenizer') # Try to remove Fast?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8ab2c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28604 new tokens to add.\n"
     ]
    }
   ],
   "source": [
    "pretrained_vocab = set(pretrained_tokenizer.get_vocab().keys())\n",
    "custom_vocab = set(custom_tokenizer.get_vocab().keys())\n",
    "\n",
    "# Identify tokens present in custom but not in pretrained\n",
    "new_tokens = list(custom_vocab - pretrained_vocab)\n",
    "\n",
    "print(f\"Found {len(new_tokens)} new tokens to add.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a46cfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 28604 new tokens.\n"
     ]
    }
   ],
   "source": [
    "# Add new tokens to the pretrained tokenizer\n",
    "num_added = pretrained_tokenizer.add_tokens(new_tokens)\n",
    "print(f\"Successfully added {num_added} new tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "177c29cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(78869, 768, padding_idx=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(pretrained_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96ed77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 749,583\n",
      "Total parameters: 147,376,621\n",
      "Percentage of trainable params: 0.51%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): RobertaForSequenceClassificationWithCustomHead(\n",
       "      (roberta): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(78869, 768, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSdpaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): RobertaClassificationHead(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=15, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): RobertaClassificationHead(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=15, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (custom_layer): Sequential(\n",
       "        (0): Linear(in_features=15, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=64, out_features=15, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,     # or SEQ_2_SEQ_LM, TOKEN_CLS, etc.\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"],  # Adjust for RoBERTa\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4c03425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.read_pickle('./saved_data/encoded_data.pck')\n",
    "le = LabelEncoder()\n",
    "data['target'] = le.fit_transform(data['Attack_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9b43492",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompt_encoded_data.txt\",\"w\") as f:\n",
    "    for _, row in data.iterrows():\n",
    "        f.write(f\"You are an expert in network traffic classification. Based on the provided network traffic attributes, you must determine whether the traffic is \" +\n",
    "                f\"'Backdoor', 'DDoS_HTTP', 'DDoS_ICMP', 'DDoS_TCP', 'DDoS_UDP', 'Fingerprinting', 'MITM', 'Normal', 'Password', 'Port_Scanning', 'Ransomware', 'SQL_injection', 'Uploading', \" + \n",
    "                f\"'Vulnerability_scanner', or 'XSS'. Here are the encoded attributes, 'encoded_PPFLE: {row['encoded_PPFLE']}'. \" + '\\n')\n",
    "#'<s>','<pad>','</s>','<unk>','<mask>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2841cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded_PPFLE_prompt</th>\n",
       "      <th>Attack_type</th>\n",
       "      <th>Attack_label</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25553</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>SQL_injection</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88566</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>Backdoor</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98685</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>XSS</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34481</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>DDoS_HTTP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54820</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>Password</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100550</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>XSS</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90973</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>Backdoor</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48624</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>DDoS_TCP</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7717</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>Ransomware</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87950</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>Backdoor</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64768</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>Port_Scanning</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6763</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>Ransomware</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107904</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7017</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>Ransomware</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151481</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>DDoS_ICMP</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     encoded_PPFLE_prompt    Attack_type  \\\n",
       "25553   You are an expert in network traffic classific...  SQL_injection   \n",
       "88566   You are an expert in network traffic classific...       Backdoor   \n",
       "98685   You are an expert in network traffic classific...            XSS   \n",
       "34481   You are an expert in network traffic classific...      DDoS_HTTP   \n",
       "54820   You are an expert in network traffic classific...       Password   \n",
       "100550  You are an expert in network traffic classific...            XSS   \n",
       "90973   You are an expert in network traffic classific...       Backdoor   \n",
       "48624   You are an expert in network traffic classific...       DDoS_TCP   \n",
       "7717    You are an expert in network traffic classific...     Ransomware   \n",
       "87950   You are an expert in network traffic classific...       Backdoor   \n",
       "64768   You are an expert in network traffic classific...  Port_Scanning   \n",
       "6763    You are an expert in network traffic classific...     Ransomware   \n",
       "107904  You are an expert in network traffic classific...         Normal   \n",
       "7017    You are an expert in network traffic classific...     Ransomware   \n",
       "151481  You are an expert in network traffic classific...      DDoS_ICMP   \n",
       "\n",
       "        Attack_label  target  \n",
       "25553              1      11  \n",
       "88566              1       0  \n",
       "98685              1      14  \n",
       "34481              1       1  \n",
       "54820              1       8  \n",
       "100550             1      14  \n",
       "90973              1       0  \n",
       "48624              1       3  \n",
       "7717               1      10  \n",
       "87950              1       0  \n",
       "64768              1       9  \n",
       "6763               1      10  \n",
       "107904             0       7  \n",
       "7017               1      10  \n",
       "151481             1       2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"prompt_encoded_data.txt\", \"r\") as f:\n",
    "    prompt_lines = f.readlines()\n",
    "\n",
    "# Strip newline characters\n",
    "prompt_lines = [prompt_line.strip() for prompt_line in prompt_lines]\n",
    "prompt_data = {\n",
    "    \"encoded_PPFLE_prompt\": prompt_lines, 'Attack_type': data['Attack_type'], 'Attack_label': data['Attack_label'], 'target': data['target']\n",
    "}\n",
    "prompt_df = pd.DataFrame(prompt_data)\n",
    "prompt_df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ccdbe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110460, 4) (23670, 4) (23670, 4)\n"
     ]
    }
   ],
   "source": [
    "train_set = prompt_df.sample(frac=0.7,random_state=42).reset_index(drop=True)\n",
    "\n",
    "remaining = prompt_df.drop(train_set.index).reset_index(drop=True)\n",
    "\n",
    "test_set = remaining.sample(frac=0.5,random_state=42).reset_index(drop=True)\n",
    "\n",
    "val_set = remaining.drop(test_set.index).reset_index(drop=True)\n",
    "\n",
    "print(train_set.shape,val_set.shape,test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59a459bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "train_set, test_set = train_test_split(prompt_df, test_size=test_ratio,stratify=prompt_df.iloc[:,-1], random_state=42)\n",
    "train_set, val_set = train_test_split(train_set, test_size=val_ratio/(val_ratio+train_ratio),stratify=train_set.iloc[:,-1], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07865133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((110460, 4), (23670, 4), (23670, 4))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape,val_set.shape,test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "184962cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LIST = ['Backdoor', 'DDoS_HTTP', 'DDoS_ICMP', 'DDoS_TCP', 'DDoS_UDP',\n",
    "                'Fingerprinting', 'MITM', 'Normal', 'Password', 'Port_Scanning',\n",
    "                'Ransomware', 'SQL_injection', 'Uploading', 'Vulnerability_scanner',\n",
    "                'XSS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "413fc530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded_PPFLE_prompt</th>\n",
       "      <th>Attack_type</th>\n",
       "      <th>Attack_label</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13140</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>Uploading</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13141</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>Uploading</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13142</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>Uploading</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13143</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>Uploading</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13144</th>\n",
       "      <td>You are an expert in network traffic classific...</td>\n",
       "      <td>Uploading</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    encoded_PPFLE_prompt Attack_type  \\\n",
       "13140  You are an expert in network traffic classific...   Uploading   \n",
       "13141  You are an expert in network traffic classific...   Uploading   \n",
       "13142  You are an expert in network traffic classific...   Uploading   \n",
       "13143  You are an expert in network traffic classific...   Uploading   \n",
       "13144  You are an expert in network traffic classific...   Uploading   \n",
       "\n",
       "       Attack_label  target  \n",
       "13140             1      12  \n",
       "13141             1      12  \n",
       "13142             1      12  \n",
       "13143             1      12  \n",
       "13144             1      12  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_df[prompt_df['Attack_type']=='Uploading'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b0006",
   "metadata": {},
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self,encodings,df,max_len):\n",
    "    self.encodings = encodings\n",
    "    self.df = df\n",
    "    self.max_len=max_len\n",
    "    self.targets = self.df['target'].tolist()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    target = self.targets[idx]\n",
    "    encoding = self.encodings[idx]\n",
    "\n",
    "    return {\n",
    "        'input_ids':encoding['input_ids'].flatten(),\n",
    "        'attention_mask':encoding['attention_mask'].flatten(),\n",
    "        'targets':torch.tensor(target,dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc04197c",
   "metadata": {},
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import RobertaTokenizerFast\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and dataset\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(row):\n",
    "    return tokenizer(\n",
    "        row[\"encoded_PPFLE_prompt\"],               # replace with your actual column\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Tokenize entire dataset\n",
    "train_set_enc = (Dataset.from_pandas(train_set)).map(tokenize_function, batched=True)\n",
    "val_set_enc = (Dataset.from_pandas(val_set)).map(tokenize_function, batched=True)\n",
    "test_set_enc = (Dataset.from_pandas(test_set)).map(tokenize_function, batched=True)\n",
    "\n",
    "# Save to disk (recommended)\n",
    "train_set_enc.save_to_disk('vera_prompt_roberta_train_encodings.pt')\n",
    "val_set_enc.save_to_disk('vera_prompt_roberta_val_encodings.pt')\n",
    "test_set_enc.save_to_disk('vera_prompt_roberta_test_encodings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09c34ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110460\n",
      "23670\n",
      "23670\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "train_set_enc = load_from_disk('vera_prompt_roberta_train_encodings.pt')\n",
    "print(len(train_set_enc))\n",
    "val_set_enc = load_from_disk('vera_prompt_roberta_val_encodings.pt')\n",
    "print(len(val_set_enc))\n",
    "test_set_enc = load_from_disk('vera_prompt_roberta_test_encodings.pt')\n",
    "print(len(test_set_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9617f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dataset_format(dataset):\n",
    "    dataset = dataset.rename_column('target', \"labels\")\n",
    "    dataset.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"]  # include \"labels\" column\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "train_dataset = set_dataset_format(train_set_enc)\n",
    "val_dataset = set_dataset_format(val_set_enc)\n",
    "test_dataset = set_dataset_format(test_set_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5dd4492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n",
      "Labels min, max: (tensor(0), tensor(14))\n",
      "Labels dtype: torch.int64\n",
      "batch.keys = dict_keys(['labels', 'input_ids', 'attention_mask']), \n",
      "tensor([    0,  1185,    32,    41,  3827,    11,  1546,  1703, 20257,     4,\n",
      "         7253,    15,     5,  1286,  1546,  1703, 16763,     6,    47,   531,\n",
      "         3094,   549,     5,  1703,    16,   128, 19085, 11219,  3934,   128,\n",
      "          495, 41501,  1215, 49244,  3934,   128,   495, 41501,  1215,  2371,\n",
      "         7629,  3934,   128,   495, 41501,  1215,  6078,   510,  3934,   128,\n",
      "          495, 41501,  1215,   791,  5174,  3934,   128,   597,  6082, 17265,\n",
      "          154,  3934,   128, 36548,   448,  3934,   128, 45647,  3934,   128,\n",
      "        44917,  3934,   128, 22117,  1215, 42450,  3509,  3934,   128,   500,\n",
      "        32387, 10680,  3934,   128, 46608,  1215,   179, 35892,  3934,   128,\n",
      "        48800,   154,  3934,   128,   846, 45706,  1215, 43511,  1396,  3934,\n",
      "           50,   128,  1000,  8108,  2652,  1398,    32,     5, 45320, 16763,\n",
      "            6,   128, 14210, 31819,  1215,  5756,   597,  3850,    35,  4285,\n",
      "         1225,   506,  2890,  5220,  2517,   506,   306,   417,  6750,   438,\n",
      "        39125,   417, 34248,  2890,  5677,   438, 26868,  1610,  3118, 33845,\n",
      "         1360,   231, 36807,   398,   417,   466, 34836,  3367,   698, 36028,\n",
      "          406,   102,  4111,   438, 40391,   102,  3761, 12010,   288, 19911,\n",
      "          398, 14141,   288, 25484,   401, 34764,   204, 38925,  3367, 32004,\n",
      "        26866, 25484,   288,   438,   466,   625,   428,  1646, 12010,   245,\n",
      "         7904,   401,   506, 32088,  3414,  3103,   417,   246,   242,  9724,\n",
      "         3204,  1749,  5208,  3145, 38504,  2518,  5352, 23219,  5479,   417,\n",
      "          134,   196,   401, 14650,   288, 12010, 30548, 28690,   306,   428,\n",
      "          246,   741,  3367,  1244,   102,  5208, 26866,  1755,  7904,  4563,\n",
      "        40027, 34682,   428, 11893,   246,   428, 40976,   102,  3414,  1922,\n",
      "          242,   306,   290,   417, 37383,   102,   306,   428,  4124,   438,\n",
      "          246, 35470,  4015,   417,   612,  7068,   176,  1942,   245, 19911,\n",
      "        20695,  4671,  4708, 19911,  5208,  6478, 36922,   506, 33845, 35470,\n",
      "         1610, 29221,   428,   398, 36431, 36028,   401,   506,   176,   102,\n",
      "         5243,   417,   406,  1978,  5606,   102, 31276,   242, 37401,   741,\n",
      "         3083,  3367, 32532,   506,  4283, 12010,   245,   102,  4550,   288,\n",
      "        16134,   246,   506,  4280,   428,   306,   242,  3305,   438,  4419,\n",
      "        14141,   506,  4283, 19911,   306,   856,  6750,   417,   466,  3706,\n",
      "         3248,  1922,  2518,  5067,   134,   102,   306,   428,  1570,  4956,\n",
      "          417,  5067,   428,  5339, 36920, 26634,   401,   242,   306,   242,\n",
      "         1366,  7589,  4791,   406,   428,  4015,   438,  4450,  4708, 36028,\n",
      "          612,  5220,   417,   466,   438,   245,  4550,  1549,  4006,   506,\n",
      "         3761,  3416,  1755,   246,   196,   364,   102,  1244,   102,  1225,\n",
      "         3209,   466,   438,  6361,   428,   406,   417,  3083,  1244,  2518,\n",
      "         1866,  5208, 17452,   401, 23219,  6668,  3209,  5379,   428,   856,\n",
      "         4124,   246,   428,   245,   438,  3103, 28503,   102,  3103,  2001,\n",
      "        25423,   176,   438,   288,  7068,  4280,   102,  3414,   438, 39617,\n",
      "         1646,  4550,  4015,   883, 30586,   102,   288,  3178,   398,   102,\n",
      "        30003,   506,   176, 23219,  5677,  1225,  3204,   246,   438,   306,\n",
      "         1610,  3416, 34836,  1646, 29936,   417,   406,   364,   176, 36807,\n",
      "         3118, 36807,  1646,  1043,   417,  3305,  1922,   428,  2001,  7309,\n",
      "        14141,   288,   428,   398,   102, 30042,  3204,   245,   242,   466,\n",
      "         1570,  3209,   155, 33492, 39259,   428, 34490,   102,  6551,   996,\n",
      "         1225,  3714,  3178,  3305,  4718, 34986,  1558,   242,  3416,  3209,\n",
      "          417,   398,  4671,  4268,  2890,   417,   245,   417,   996,  5208,\n",
      "         3272,   242,   698, 28020,   844, 10056,  4671,   428,   246,   242,\n",
      "         1570,  1755, 32976,   417,   406,  3209,  7678, 37413, 38133,   245,\n",
      "          438,   401,  3178,  1558,   102,   288,   196,  3367,  2481,   438,\n",
      "          398,     2])\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN=512\n",
    "BATCH_SIZE=32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0\n",
    "\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0\n",
    "\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0\n",
    "\n",
    ")\n",
    "\n",
    "test_data = next(iter(train_loader))\n",
    "\n",
    "print(test_data['input_ids'].shape)\n",
    "\n",
    "labels = train_dataset[\"labels\"]\n",
    "print(f\"Labels min, max: {labels.min(), labels.max()}\")\n",
    "print(f\"Labels dtype: {labels.dtype}\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(f\"batch.keys = {batch.keys()}, \")\n",
    "    break\n",
    "\n",
    "print(train_dataset['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1800dc26",
   "metadata": {},
   "source": [
    "for batch in dataloader:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68959c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chkpt(model,version):\n",
    "  return model.load_state_dict(torch.load(f\"./saved_model/securityRoBERTa{version}.0.pt\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526288b9",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_model_peft(trainer,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n",
    "  trainer.model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  for d in data_loader:\n",
    "    input_ids = d['input_ids'].to(device)\n",
    "    attention_mask = d['attention_mask'].to(device)\n",
    "    labels = d[\"labels\"].to(device)\n",
    "    \n",
    "    outputs = trainer.model(input_ids,attention_mask, labels=labels)\n",
    "    logits = outputs.logits  # extract the logits tensor\n",
    "    _,preds = torch.max(logits,dim=1)\n",
    "    loss = loss_fn(logits,labels)\n",
    "\n",
    "    correct_predictions+=torch.sum(preds==labels).cpu()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(trainer.model.parameters(),max_norm=1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions/n_examples,np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70471911",
   "metadata": {},
   "source": [
    "def evaluation_model_peft(trainer,data_loader,loss_fn,device,n_examples):\n",
    "  trainer.model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  for d in data_loader:\n",
    "    input_ids = d['input_ids'].to(device)\n",
    "    attention_mask = d['attention_mask'].to(device)\n",
    "    labels = d['label'].to(device)\n",
    "\n",
    "    outputs = trainer.model(input_ids,attention_mask, labels=labels)\n",
    "    logits = outputs.logits  # extract the logits tensor\n",
    "    _,preds = torch.max(logits,dim=1)\n",
    "    loss = loss_fn(logits,labels)\n",
    "\n",
    "    correct_predictions+=torch.sum(preds==labels).cpu()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions/n_examples,np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9e6aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.epoch_train_losses = []\n",
    "        self.epoch_train_accuracies = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is None:\n",
    "            return\n",
    "        \n",
    "        if \"loss\" in logs:\n",
    "            self.epoch_train_losses.append(logs[\"loss\"])\n",
    "\n",
    "        if \"eval_accuracy\" in logs:\n",
    "            self.epoch_train_accuracies.append(logs[\"eval_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87687bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.history = defaultdict(list)\n",
    "        self.best_val_acc = 0.0\n",
    "\n",
    "    def evaluate_and_save(self, epoch, model_version):\n",
    "        # Evaluate on validation set\n",
    "        metrics = self.evaluate()\n",
    "        print(metrics.keys())\n",
    "        val_acc = metrics[\"eval_accuracy\"]\n",
    "        val_loss = metrics[\"eval_loss\"]\n",
    "        self.history[\"val_acc\"].append(val_acc)\n",
    "        self.history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = val_acc\n",
    "            self.save_model(f\"./saved_model/{model_version}{epoch+1}.0.pt\")\n",
    "            print(f\"Saved best model at epoch {epoch+1} with val_acc={val_acc:.4f}\")\n",
    "\n",
    "    def log_training_epoch(self, train_loss, train_acc):\n",
    "        self.history[\"train_loss\"].append(train_loss)\n",
    "        self.history[\"train_acc\"].append(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d885a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=3\n",
    "optimizer_prompt_vera = torch.optim.AdamW(model.parameters(),lr=1e-5)\n",
    "total_steps = len(train_loader)*EPOCHS\n",
    "\n",
    "scheduler_prompt_vera = get_linear_schedule_with_warmup(\n",
    "    optimizer_prompt_vera,\n",
    "    num_warmup_steps= 0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2b27086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# output dir \n",
    "model_version = \"securityRoBERTa_BasePromptVeRA_\"\n",
    "model_dir = f\"{model_version}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    run_name=model_version,\n",
    "    output_dir=model_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=1,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    logging_dir=f\"{model_dir}/logs\",\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    dataloader_num_workers=4,  # Adjust based on your CPU capabilities\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing to save memory\n",
    "    report_to=\"none\",  # Disable reporting to avoid unnecessary overhead\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b33e86fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 749,583\n",
      "Total parameters: 147,376,621\n",
      "Percentage of trainable params: 0.51%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# configure Trainer\\ntrainer_prompt_vera = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=train_dataset,\\n    eval_dataset=val_dataset,\\n    compute_metrics=compute_metrics\\n)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, confusion_matrix\n",
    "# The parameters after appling LoRA\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "\"\"\"\n",
    "# designing computing metrics as per our use case. (F1-Macro is essential and log-loss is optional)\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p.predictions, TARGET_LIST\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(TARGET_LIST, predictions)\n",
    "    macro_f1 = f1_score(TARGET_LIST, predictions, average='macro')\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"macro_f1\": macro_f1}\n",
    "\"\"\"\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"macro_f1\": macro_f1}\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\"\"\"\n",
    "# configure Trainer\n",
    "trainer_prompt_vera = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebc9bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61746313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='863' max='863' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [863/863 07:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.814500</td>\n",
       "      <td>2.688004</td>\n",
       "      <td>0.153992</td>\n",
       "      <td>0.017792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1480' max='740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [740/740 08:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['eval_loss', 'eval_accuracy', 'eval_macro_f1', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
      "Saved best model at epoch 1 with val_acc=0.1540\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='863' max='863' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [863/863 07:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.726400</td>\n",
       "      <td>2.670849</td>\n",
       "      <td>0.153992</td>\n",
       "      <td>0.017792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1480' max='740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [740/740 09:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['eval_loss', 'eval_accuracy', 'eval_macro_f1', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='863' max='863' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [863/863 08:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.670700</td>\n",
       "      <td>2.661056</td>\n",
       "      <td>0.153992</td>\n",
       "      <td>0.017792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='740' max='740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [740/740 01:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['eval_loss', 'eval_accuracy', 'eval_macro_f1', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
      "Training complete and history saved.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "metrics_callback = MetricsCallback()\n",
    "\n",
    "# New trainer\n",
    "trainer_prompt_vera = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[metrics_callback]\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    # Train for one epoch (Trainer handles batches internally)\n",
    "    trainer_prompt_vera.train()\n",
    "\n",
    "    trainer_prompt_vera.log_training_epoch(metrics_callback.epoch_train_losses, metrics_callback.epoch_train_accuracies)\n",
    "\n",
    "    # Evaluate and potentially save best model\n",
    "    trainer_prompt_vera.evaluate_and_save(\n",
    "        epoch,\n",
    "        model_version\n",
    "    )\n",
    "\n",
    "history_roberta_base_prompt_vera = trainer_prompt_vera.history\n",
    "\n",
    "# Save full history and training time\n",
    "end_time = time.time()\n",
    "history_roberta_base_prompt_vera[\"training_time\"].append(end_time - start_time)\n",
    "\n",
    "torch.save(history_roberta_base_prompt_vera, \"./saved_model/history_roberta_base_prompt_vera.pt\")\n",
    "print(\"Training complete and history saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6233da18",
   "metadata": {},
   "source": [
    "%%time\n",
    "history_roberta_base_prompt_vera = defaultdict(list)\n",
    "best_accuracy_roberta_base_prompt_vera=0\n",
    "print(\"1\")\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "  print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "  train_acc_roberta_base_prompt_vera,train_loss_roberta_base_prompt_vera = train_model_peft(trainer_prompt_vera,train_loader,loss_fn,optimizer_prompt_vera,device,scheduler_prompt_vera,len(train_set))\n",
    "  val_acc_roberta_base_prompt_vera,val_loss_roberta_base_prompt_vera = evaluation_model_peft(trainer_prompt_vera,val_loader,loss_fn,device,len(val_set))\n",
    "  history_roberta_base_prompt_vera['train_acc'].append(train_acc_roberta_base_prompt_vera)\n",
    "  history_roberta_base_prompt_vera['train_loss'].append(train_loss_roberta_base_prompt_vera)\n",
    "  history_roberta_base_prompt_vera['val_acc'].append(val_acc_roberta_base_prompt_vera)\n",
    "  history_roberta_base_prompt_vera['val_loss'].append(val_loss_roberta_base_prompt_vera)\n",
    "  print(f\"Train Loss {train_loss_roberta_base_prompt_vera} | Validation Loss {val_loss_roberta_base_prompt_vera} | Training Accuracy {train_acc_roberta_base_prompt_vera} | Validation Accuracy {val_acc_roberta_base_prompt_vera}\")\n",
    "\n",
    "  if val_acc_roberta_base_prompt_vera>best_accuracy_roberta_base_prompt_vera:\n",
    "    trainer_prompt_vera.save_model(f\"./saved_model/{model_version}{epoch+1}.0.pt\")\n",
    "    best_accuracy_roberta_base_prompt_vera = val_acc_roberta_base_prompt_vera\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate training time\n",
    "history_roberta_base_prompt_vera['training_time'].append(end_time - start_time)\n",
    "\n",
    "# Save\n",
    "torch.save(history_roberta_base_prompt_vera, \"./saved_model/history_roberta_base_prompt_vera.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6381f8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.defaultdict'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "history_roberta_base_prompt_vera = torch.load(\"./saved_model/history_roberta_base_prompt_vera.pt\", weights_only=False)\n",
    "\n",
    "# Still a defaultdict with tensors\n",
    "print(type(history_roberta_base_prompt_vera))  # defaultdict\n",
    "print(type(history_roberta_base_prompt_vera['train_acc'][0]))  # torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33331f85",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\seaborn\\_base.py:1768\u001b[0m, in \u001b[0;36mcategorical_order\u001b[1;34m(vector, order)\u001b[0m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1768\u001b[0m     order \u001b[38;5;241m=\u001b[39m vector\u001b[38;5;241m.\u001b[39mcat\u001b[38;5;241m.\u001b[39mcategories\n\u001b[0;32m   1769\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\pandas\\core\\generic.py:6299\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\pandas\\core\\accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[1;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor(obj)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:2898\u001b[0m, in \u001b[0;36mCategoricalAccessor.__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   2897\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2898\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate(data)\n\u001b[0;32m   2899\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:2907\u001b[0m, in \u001b[0;36mCategoricalAccessor._validate\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   2906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype):\n\u001b[1;32m-> 2907\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .cat accessor with a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .cat accessor with a 'category' dtype",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m val_losses \u001b[38;5;241m=\u001b[39m history_roberta_base_prompt_vera[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m---> 10\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot(x\u001b[38;5;241m=\u001b[39mepochs, y\u001b[38;5;241m=\u001b[39mtrain_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m,marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot(x\u001b[38;5;241m=\u001b[39mepochs, y\u001b[38;5;241m=\u001b[39mval_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m,marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRoBERTa Base VeRA Prompt Training and Validation Losses\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\seaborn\\relational.py:508\u001b[0m, in \u001b[0;36mlineplot\u001b[1;34m(data, x, y, hue, size, style, units, weights, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, estimator, errorbar, n_boot, seed, orient, sort, err_style, err_kws, legend, ci, ax, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p\u001b[38;5;241m.\u001b[39mhas_xy_data:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ax\n\u001b[1;32m--> 508\u001b[0m p\u001b[38;5;241m.\u001b[39m_attach(ax)\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Other functions have color as an explicit param,\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;66;03m# and we should probably do that here too\u001b[39;00m\n\u001b[0;32m    512\u001b[0m color \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\seaborn\\_base.py:1134\u001b[0m, in \u001b[0;36mVectorPlotter._attach\u001b[1;34m(self, obj, allowed_types, log_scale)\u001b[0m\n\u001b[0;32m   1132\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m                 order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1134\u001b[0m             seed_data \u001b[38;5;241m=\u001b[39m categorical_order(seed_data, order)\n\u001b[0;32m   1135\u001b[0m         converter\u001b[38;5;241m.\u001b[39mupdate_units(seed_data)\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# -- Set numerical axis scales\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# First unpack the log_scale argument\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\seaborn\\_base.py:1771\u001b[0m, in \u001b[0;36mcategorical_order\u001b[1;34m(vector, order)\u001b[0m\n\u001b[0;32m   1768\u001b[0m     order \u001b[38;5;241m=\u001b[39m vector\u001b[38;5;241m.\u001b[39mcat\u001b[38;5;241m.\u001b[39mcategories\n\u001b[0;32m   1769\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[1;32m-> 1771\u001b[0m     order \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(vector)\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m variable_type(vector) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1774\u001b[0m         order \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msort(order)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\pandas\\core\\series.py:2407\u001b[0m, in \u001b[0;36mSeries.unique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[0;32m   2345\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2346\u001b[0m \u001b[38;5;124;03m    Return unique values of Series object.\u001b[39;00m\n\u001b[0;32m   2347\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2405\u001b[0m \u001b[38;5;124;03m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[0;32m   2406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39munique()\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\pandas\\core\\base.py:1025\u001b[0m, in \u001b[0;36mIndexOpsMixin.unique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1025\u001b[0m     result \u001b[38;5;241m=\u001b[39m algorithms\u001b[38;5;241m.\u001b[39munique1d(values)\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\pandas\\core\\algorithms.py:401\u001b[0m, in \u001b[0;36munique\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(values):\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unique_with_mask(values)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\pandas\\core\\algorithms.py:440\u001b[0m, in \u001b[0;36munique_with_mask\u001b[1;34m(values, mask)\u001b[0m\n\u001b[0;32m    438\u001b[0m table \u001b[38;5;241m=\u001b[39m hashtable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 440\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39munique(values)\n\u001b[0;32m    441\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uniques\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7248\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.unique\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7195\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "epochs = range(1, EPOCHS+1)\n",
    "train_losses = history_roberta_base_prompt_vera['train_loss']\n",
    "val_losses = history_roberta_base_prompt_vera['val_loss']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.lineplot(x=epochs, y=train_losses, label='Training Loss',marker=\"o\")\n",
    "sns.lineplot(x=epochs, y=val_losses, label='Validation Loss',marker=\"o\")\n",
    "\n",
    "plt.title('RoBERTa Base VeRA Prompt Training and Validation Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('./roberta-base-prompt-vera-training-validation-losses.png',dpi=780)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1febc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, EPOCHS+1)\n",
    "train_accuracies = [tens.item() for tens in history_roberta_base_prompt_vera['train_acc']]\n",
    "val_accuracies = [tens.item() for tens in history_roberta_base_prompt_vera['val_acc']]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.lineplot(x=epochs, y=train_accuracies, label='Training accuracy',marker='o')\n",
    "sns.lineplot(x=epochs, y=val_accuracies, label='Validation accuracy',marker='o')\n",
    "\n",
    "plt.title('RoBERTa Base VeRA Prompt Training and Validation Accuracies')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('./roberta-base-prompt-vera-training-validation-accuracies.png',dpi=780)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d803d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import PredictionOutput\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "def get_predictions(trainer, test_dataset):\n",
    "    # Runs prediction\n",
    "    output: PredictionOutput = trainer.predict(test_dataset)\n",
    "\n",
    "    # Get raw logits and labels\n",
    "    logits = torch.tensor(output.predictions)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "    labels = torch.tensor(output.label_ids)\n",
    "\n",
    "    return preds, probs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3accecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, probs, labels = get_predictions(trainer_prompt_vera, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4510d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(preds,\"./saved_model/roberta_base_prompt_vera_predictions.pt\")\n",
    "torch.save(probs,\"./saved_model/roberta_base_prompt_vera_predictions_probs.pt\")\n",
    "torch.save(labels,\"./saved_model/roberta_base_prompt_vera_real_values.pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee6873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Backdoor', 'DDoS_HTTP', 'DDoS_ICMP', 'DDoS_TCP', 'DDoS_UDP',\n",
    "       'Fingerprinting', 'MITM', 'Normal', 'Password', 'Port_Scanning',\n",
    "       'Ransomware', 'SQL_injection', 'Uploading', 'Vulnerability_scanner',\n",
    "       'XSS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a0e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in preds:\n",
    "  s.add(elt.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6825de",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_considered_classes = [TARGET_LIST[i] for i in s]\n",
    "actual_considered_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Attack_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e02f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='d')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"RoBERTa Base VeRA Prompt Confusion Matrix\")\n",
    "    plt.ylabel('Real threats')\n",
    "    plt.xlabel('Predicted threats')\n",
    "    plt.savefig('./roberta-base-prompt-vera-confusion-matrix.png',dpi=780)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d11d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels,preds,target_names=TARGET_LIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab68a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(labels,preds)\n",
    "df_cm = pd.DataFrame(cm,index=TARGET_LIST,columns=TARGET_LIST)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0881076",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = labels.cpu().numpy()\n",
    "y_score = preds.cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
