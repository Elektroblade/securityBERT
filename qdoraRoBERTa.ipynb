{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13462dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer,get_linear_schedule_with_warmup,RobertaTokenizer,BertForSequenceClassification,BitsAndBytesConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm.notebook import tqdm\n",
    "import psutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import get_peft_model\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e004ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable:,}\")\n",
    "    print(f\"Total parameters: {total:,}\")\n",
    "    print(f\"Percentage of trainable params: {100 * trainable / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a80d5",
   "metadata": {},
   "source": [
    "## Custom RoBERTa class needs work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b8a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "class RobertaForSequenceClassificationWithCustomHead(RobertaForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.custom_layer = nn.Sequential(\n",
    "            nn.Linear(config.num_labels, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, config.num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        accepted_keys = {\n",
    "            \"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\",\n",
    "            \"position_ids\", \"head_mask\", \"inputs_embeds\", \"output_attentions\",\n",
    "            \"output_hidden_states\", \"return_dict\"\n",
    "        }\n",
    "        filtered_kwargs = {k: v for k, v in kwargs.items() if k in accepted_keys}\n",
    "\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            **filtered_kwargs\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Move custom layer to correct device if needed\n",
    "        if next(self.custom_layer.parameters()).device != logits.device:\n",
    "            self.custom_layer.to(logits.device)\n",
    "\n",
    "        custom_logits = self.custom_layer(logits)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(custom_logits, labels)\n",
    "\n",
    "        return type(outputs)(\n",
    "            loss=loss,\n",
    "            logits=custom_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4f4d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "GPU Name: NVIDIA GeForce RTX 3080\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Parameter' object has no attribute 'SCB'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU available.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m device\n\u001b[1;32m---> 17\u001b[0m model \u001b[38;5;241m=\u001b[39m RobertaForSequenceClassificationWithCustomHead\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, quantization_config\u001b[38;5;241m=\u001b[39mbnb_config, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[0;32m     19\u001b[0m print_trainable_parameters(model)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\transformers\\modeling_utils.py:313\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\transformers\\modeling_utils.py:4680\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4671\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   4673\u001b[0m     (\n\u001b[0;32m   4674\u001b[0m         model,\n\u001b[0;32m   4675\u001b[0m         missing_keys,\n\u001b[0;32m   4676\u001b[0m         unexpected_keys,\n\u001b[0;32m   4677\u001b[0m         mismatched_keys,\n\u001b[0;32m   4678\u001b[0m         offload_index,\n\u001b[0;32m   4679\u001b[0m         error_msgs,\n\u001b[1;32m-> 4680\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[0;32m   4681\u001b[0m         model,\n\u001b[0;32m   4682\u001b[0m         state_dict,\n\u001b[0;32m   4683\u001b[0m         checkpoint_files,\n\u001b[0;32m   4684\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   4685\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[0;32m   4686\u001b[0m         sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[0;32m   4687\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[0;32m   4688\u001b[0m         disk_offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[0;32m   4689\u001b[0m         offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[0;32m   4690\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[0;32m   4691\u001b[0m         hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[0;32m   4692\u001b[0m         keep_in_fp32_regex\u001b[38;5;241m=\u001b[39mkeep_in_fp32_regex,\n\u001b[0;32m   4693\u001b[0m         device_mesh\u001b[38;5;241m=\u001b[39mdevice_mesh,\n\u001b[0;32m   4694\u001b[0m         key_mapping\u001b[38;5;241m=\u001b[39mkey_mapping,\n\u001b[0;32m   4695\u001b[0m         weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[0;32m   4696\u001b[0m     )\n\u001b[0;32m   4698\u001b[0m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[0;32m   4699\u001b[0m model\u001b[38;5;241m.\u001b[39m_tp_size \u001b[38;5;241m=\u001b[39m tp_size\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\transformers\\modeling_utils.py:4990\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[0;32m   4987\u001b[0m model\u001b[38;5;241m.\u001b[39m_move_missing_keys_from_meta_to_cpu(missing_keys \u001b[38;5;241m+\u001b[39m mismatched_keys, unexpected_keys, dtype, hf_quantizer)\n\u001b[0;32m   4989\u001b[0m \u001b[38;5;66;03m# correctly initialize the missing (and potentially mismatched) keys\u001b[39;00m\n\u001b[1;32m-> 4990\u001b[0m model\u001b[38;5;241m.\u001b[39m_initialize_missing_keys(checkpoint_keys, ignore_mismatched_sizes, is_quantized)\n\u001b[0;32m   4992\u001b[0m \u001b[38;5;66;03m# Set some modules to fp32 if needed\u001b[39;00m\n\u001b[0;32m   4993\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_in_fp32_regex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\transformers\\modeling_utils.py:5533\u001b[0m, in \u001b[0;36mPreTrainedModel._initialize_missing_keys\u001b[1;34m(self, loaded_keys, ignore_mismatched_sizes, is_quantized)\u001b[0m\n\u001b[0;32m   5527\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the missing keys (keys that are part of the model parameters, but were NOT found in the loaded state dicts), according to\u001b[39;00m\n\u001b[0;32m   5528\u001b[0m \u001b[38;5;124;03m`_initialize_weights`. Indeed, since the corresponding weights are missing from the state dict, they will not be replaced and need to\u001b[39;00m\n\u001b[0;32m   5529\u001b[0m \u001b[38;5;124;03mbe initialized correctly (i.e. weight initialization distribution).\u001b[39;00m\n\u001b[0;32m   5530\u001b[0m \u001b[38;5;124;03mAlso take care of setting the `_is_hf_initialized` flag for keys that are not missing.\u001b[39;00m\n\u001b[0;32m   5531\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_mismatched_sizes:\n\u001b[1;32m-> 5533\u001b[0m     not_initialized_submodules \u001b[38;5;241m=\u001b[39m set_initialized_submodules(\u001b[38;5;28mself\u001b[39m, loaded_keys)\n\u001b[0;32m   5534\u001b[0m     \u001b[38;5;66;03m# If we're about to tie the output embeds to the input embeds we don't need to init them\u001b[39;00m\n\u001b[0;32m   5535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5536\u001b[0m         \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_text_config(decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_word_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5537\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_text_config(decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mtie_word_embeddings\n\u001b[0;32m   5538\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\transformers\\modeling_utils.py:614\u001b[0m, in \u001b[0;36mset_initialized_submodules\u001b[1;34m(model, state_dict_keys)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module_name, module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules():\n\u001b[0;32m    612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    613\u001b[0m         \u001b[38;5;66;03m# When checking if the root module is loaded there's no need to prepend module_name.\u001b[39;00m\n\u001b[1;32m--> 614\u001b[0m         module_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(module\u001b[38;5;241m.\u001b[39mstate_dict())\n\u001b[0;32m    615\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    616\u001b[0m         module_keys \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mstate_dict()}\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2228\u001b[0m, in \u001b[0;36mModule.state_dict\u001b[1;34m(self, destination, prefix, keep_vars, *args)\u001b[0m\n\u001b[0;32m   2226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2228\u001b[0m         module\u001b[38;5;241m.\u001b[39mstate_dict(\n\u001b[0;32m   2229\u001b[0m             destination\u001b[38;5;241m=\u001b[39mdestination,\n\u001b[0;32m   2230\u001b[0m             prefix\u001b[38;5;241m=\u001b[39mprefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2231\u001b[0m             keep_vars\u001b[38;5;241m=\u001b[39mkeep_vars,\n\u001b[0;32m   2232\u001b[0m         )\n\u001b[0;32m   2233\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m   2234\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, destination, prefix, local_metadata)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2228\u001b[0m, in \u001b[0;36mModule.state_dict\u001b[1;34m(self, destination, prefix, keep_vars, *args)\u001b[0m\n\u001b[0;32m   2226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2228\u001b[0m         module\u001b[38;5;241m.\u001b[39mstate_dict(\n\u001b[0;32m   2229\u001b[0m             destination\u001b[38;5;241m=\u001b[39mdestination,\n\u001b[0;32m   2230\u001b[0m             prefix\u001b[38;5;241m=\u001b[39mprefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2231\u001b[0m             keep_vars\u001b[38;5;241m=\u001b[39mkeep_vars,\n\u001b[0;32m   2232\u001b[0m         )\n\u001b[0;32m   2233\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m   2234\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, destination, prefix, local_metadata)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2225\u001b[0m, in \u001b[0;36mModule.state_dict\u001b[1;34m(self, destination, prefix, keep_vars, *args)\u001b[0m\n\u001b[0;32m   2223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict_pre_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m   2224\u001b[0m     hook(\u001b[38;5;28mself\u001b[39m, prefix, keep_vars)\n\u001b[1;32m-> 2225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_to_state_dict(destination, prefix, keep_vars)\n\u001b[0;32m   2226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:939\u001b[0m, in \u001b[0;36mLinear8bitLt._save_to_state_dict\u001b[1;34m(self, destination, prefix, keep_vars)\u001b[0m\n\u001b[0;32m    936\u001b[0m scb_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCB\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;66;03m# case 1: .cuda was called, SCB is in self.weight\u001b[39;00m\n\u001b[1;32m--> 939\u001b[0m param_from_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, scb_name)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;66;03m# case 2: self.init_8bit_state was called, SCB is in self.state\u001b[39;00m\n\u001b[0;32m    941\u001b[0m param_from_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, scb_name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Parameter' object has no attribute 'SCB'"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    "    bnb_8bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "device\n",
    "model = RobertaForSequenceClassificationWithCustomHead.from_pretrained(\"roberta-base\", num_labels=15, quantization_config=bnb_config, device_map=\"auto\")\n",
    "print(model.config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Custom tokenizer trained on OOL tokens\n",
    "custom_tokenizer = RobertaTokenizer.from_pretrained('./tokenizer') # Try to remove Fast?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab2c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30522 new tokens to use.\n"
     ]
    }
   ],
   "source": [
    "custom_vocab = set(custom_tokenizer.get_vocab().keys())\n",
    "\n",
    "new_tokens = list(custom_vocab)\n",
    "\n",
    "print(f\"Found {len(new_tokens)} new tokens to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c29cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(custom_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ed77f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1,072,143\n",
      "Total parameters: 68,101,357\n",
      "Percentage of trainable params: 1.57%\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,     # or SEQ_2_SEQ_LM, TOKEN_CLS, etc.\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.2,\n",
    "    target_modules=[\"query\", \"key\", \"value\"],  # Adjust for RoBERTa\n",
    "    use_dora=True\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c03425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = pd.read_pickle('./saved_data/encoded_data.pck')\n",
    "le = LabelEncoder()\n",
    "data['target'] = le.fit_transform(data['Attack_type'])\n",
    "\n",
    "data2 = pd.read_csv(\".\\data\\Edge-IIoTset dataset\\Selected dataset for ML and DL\\ML-EdgeIIoT-dataset.csv\")\n",
    "columns = list(data2.columns[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b43492",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"column_name_encoded_data.txt\",\"w\") as f:\n",
    "    for _, row in data.iterrows():\n",
    "        tokens = row['encoded_PPFLE'].split(\" \")\n",
    "        formatted = []\n",
    "        for idx, (col, tok) in enumerate(zip(columns, tokens)):\n",
    "            end_char = \".\" if idx == len(tokens) - 1 else \";\"\n",
    "            formatted.append(f\"{col}: {tok}{end_char}\")\n",
    "        row_string = \" \".join(formatted)\n",
    "        f.write(row_string + \"\\n\")\n",
    "#'<s>','<pad>','</s>','<unk>','<mask>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2841cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded_PPFLE_prompt</th>\n",
       "      <th>Attack_type</th>\n",
       "      <th>Attack_label</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132736</th>\n",
       "      <td>frame.time: 52bbee57f7584edca9f7d865ddef44d977...</td>\n",
       "      <td>DDoS_UDP</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12466</th>\n",
       "      <td>frame.time: 4997fcf6a96efbd5474a93c4d32118150f...</td>\n",
       "      <td>Ransomware</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39192</th>\n",
       "      <td>frame.time: 26ac88ea1b357cde53d46d57122f996ee6...</td>\n",
       "      <td>DDoS_HTTP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3672</th>\n",
       "      <td>frame.time: 27f16c64e331b309d5ec22bcd9e100cf70...</td>\n",
       "      <td>Ransomware</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107530</th>\n",
       "      <td>frame.time: 4196d9b45389faf499d21056daa33a761f...</td>\n",
       "      <td>Normal</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154241</th>\n",
       "      <td>frame.time: 7183de64eda221ea7d3ad7d13898372895...</td>\n",
       "      <td>DDoS_ICMP</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44393</th>\n",
       "      <td>frame.time: 17193dc56ca53e85bdd173130cfab194fa...</td>\n",
       "      <td>DDoS_TCP</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91755</th>\n",
       "      <td>frame.time: 764e556221846b2c53d3f648af5afa5ebf...</td>\n",
       "      <td>Backdoor</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53665</th>\n",
       "      <td>frame.time: 0adcf1af4c1e93e58919a108f1d6193c56...</td>\n",
       "      <td>DDoS_TCP</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84186</th>\n",
       "      <td>frame.time: dced56621bda7629676a8f06e7cd2433f5...</td>\n",
       "      <td>Vulnerability_scanner</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89888</th>\n",
       "      <td>frame.time: be1d51057d56d39b8c2dfd24a6afc25256...</td>\n",
       "      <td>Backdoor</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5860</th>\n",
       "      <td>frame.time: 615382049d04f2b8ca30b08b588892d5e2...</td>\n",
       "      <td>Ransomware</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98835</th>\n",
       "      <td>frame.time: b7fd1bd26ef446a22914a2f7ea54c94a60...</td>\n",
       "      <td>XSS</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68344</th>\n",
       "      <td>frame.time: 5f5fdb2c32f434553f7bff7e6439c48502...</td>\n",
       "      <td>Port_Scanning</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56087</th>\n",
       "      <td>frame.time: 4833d09153716ee6ac1fb7f3e6419251c9...</td>\n",
       "      <td>Password</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     encoded_PPFLE_prompt  \\\n",
       "132736  frame.time: 52bbee57f7584edca9f7d865ddef44d977...   \n",
       "12466   frame.time: 4997fcf6a96efbd5474a93c4d32118150f...   \n",
       "39192   frame.time: 26ac88ea1b357cde53d46d57122f996ee6...   \n",
       "3672    frame.time: 27f16c64e331b309d5ec22bcd9e100cf70...   \n",
       "107530  frame.time: 4196d9b45389faf499d21056daa33a761f...   \n",
       "154241  frame.time: 7183de64eda221ea7d3ad7d13898372895...   \n",
       "44393   frame.time: 17193dc56ca53e85bdd173130cfab194fa...   \n",
       "91755   frame.time: 764e556221846b2c53d3f648af5afa5ebf...   \n",
       "53665   frame.time: 0adcf1af4c1e93e58919a108f1d6193c56...   \n",
       "84186   frame.time: dced56621bda7629676a8f06e7cd2433f5...   \n",
       "89888   frame.time: be1d51057d56d39b8c2dfd24a6afc25256...   \n",
       "5860    frame.time: 615382049d04f2b8ca30b08b588892d5e2...   \n",
       "98835   frame.time: b7fd1bd26ef446a22914a2f7ea54c94a60...   \n",
       "68344   frame.time: 5f5fdb2c32f434553f7bff7e6439c48502...   \n",
       "56087   frame.time: 4833d09153716ee6ac1fb7f3e6419251c9...   \n",
       "\n",
       "                  Attack_type  Attack_label  target  \n",
       "132736               DDoS_UDP             1       4  \n",
       "12466              Ransomware             1      10  \n",
       "39192               DDoS_HTTP             1       1  \n",
       "3672               Ransomware             1      10  \n",
       "107530                 Normal             0       7  \n",
       "154241              DDoS_ICMP             1       2  \n",
       "44393                DDoS_TCP             1       3  \n",
       "91755                Backdoor             1       0  \n",
       "53665                DDoS_TCP             1       3  \n",
       "84186   Vulnerability_scanner             1      13  \n",
       "89888                Backdoor             1       0  \n",
       "5860               Ransomware             1      10  \n",
       "98835                     XSS             1      14  \n",
       "68344           Port_Scanning             1       9  \n",
       "56087                Password             1       8  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"column_name_encoded_data.txt\", \"r\") as f:\n",
    "    prompt_lines = f.readlines()\n",
    "\n",
    "# Strip newline characters\n",
    "prompt_lines = [prompt_line.strip() for prompt_line in prompt_lines]\n",
    "prompt_data = {\n",
    "    \"encoded_PPFLE_prompt\": prompt_lines, 'Attack_type': data['Attack_type'], 'Attack_label': data['Attack_label'], 'target': data['target']\n",
    "}\n",
    "prompt_df = pd.DataFrame(prompt_data)\n",
    "prompt_df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccdbe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110460, 4) (23670, 4) (23670, 4)\n"
     ]
    }
   ],
   "source": [
    "train_set = prompt_df.sample(frac=0.7,random_state=42).reset_index(drop=True)\n",
    "\n",
    "remaining = prompt_df.drop(train_set.index).reset_index(drop=True)\n",
    "\n",
    "test_set = remaining.sample(frac=0.5,random_state=42).reset_index(drop=True)\n",
    "\n",
    "val_set = remaining.drop(test_set.index).reset_index(drop=True)\n",
    "\n",
    "print(train_set.shape,val_set.shape,test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a459bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "train_set, test_set = train_test_split(prompt_df, test_size=test_ratio,stratify=prompt_df.iloc[:,-1], random_state=42)\n",
    "train_set, val_set = train_test_split(train_set, test_size=val_ratio/(val_ratio+train_ratio),stratify=train_set.iloc[:,-1], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07865133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((110460, 4), (23670, 4), (23670, 4))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape,val_set.shape,test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184962cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LIST = ['Backdoor', 'DDoS_HTTP', 'DDoS_ICMP', 'DDoS_TCP', 'DDoS_UDP',\n",
    "                'Fingerprinting', 'MITM', 'Normal', 'Password', 'Port_Scanning',\n",
    "                'Ransomware', 'SQL_injection', 'Uploading', 'Vulnerability_scanner',\n",
    "                'XSS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413fc530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded_PPFLE_prompt</th>\n",
       "      <th>Attack_type</th>\n",
       "      <th>Attack_label</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13140</th>\n",
       "      <td>frame.time: 05293357ba325a590027981a9b59eb748a...</td>\n",
       "      <td>Uploading</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13141</th>\n",
       "      <td>frame.time: da0992898db14d190c3b6f7c2c6c658259...</td>\n",
       "      <td>Uploading</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13142</th>\n",
       "      <td>frame.time: 4d5b89f0a6797f41a5d0ee5b7b1cfe1327...</td>\n",
       "      <td>Uploading</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13143</th>\n",
       "      <td>frame.time: 732fcefab7fb269fd8ae7bd4d3b3e28ec3...</td>\n",
       "      <td>Uploading</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13144</th>\n",
       "      <td>frame.time: 9a7d459b7d8a0e5c070cb85c3b1bd0824a...</td>\n",
       "      <td>Uploading</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    encoded_PPFLE_prompt Attack_type  \\\n",
       "13140  frame.time: 05293357ba325a590027981a9b59eb748a...   Uploading   \n",
       "13141  frame.time: da0992898db14d190c3b6f7c2c6c658259...   Uploading   \n",
       "13142  frame.time: 4d5b89f0a6797f41a5d0ee5b7b1cfe1327...   Uploading   \n",
       "13143  frame.time: 732fcefab7fb269fd8ae7bd4d3b3e28ec3...   Uploading   \n",
       "13144  frame.time: 9a7d459b7d8a0e5c070cb85c3b1bd0824a...   Uploading   \n",
       "\n",
       "       Attack_label  target  \n",
       "13140             1      12  \n",
       "13141             1      12  \n",
       "13142             1      12  \n",
       "13143             1      12  \n",
       "13144             1      12  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_df[prompt_df['Attack_type']=='Uploading'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b0006",
   "metadata": {},
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self,encodings,df,max_len):\n",
    "    self.encodings = encodings\n",
    "    self.df = df\n",
    "    self.max_len=max_len\n",
    "    self.targets = self.df['target'].tolist()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    target = self.targets[idx]\n",
    "    encoding = self.encodings[idx]\n",
    "\n",
    "    return {\n",
    "        'input_ids':encoding['input_ids'].flatten(),\n",
    "        'attention_mask':encoding['attention_mask'].flatten(),\n",
    "        'targets':torch.tensor(target,dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc04197c",
   "metadata": {},
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import RobertaTokenizerFast\n",
    "import torch\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(row):\n",
    "    return custom_tokenizer(\n",
    "        row[\"encoded_PPFLE_prompt\"],               # replace with your actual column\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Tokenize entire dataset\n",
    "train_set_enc = (Dataset.from_pandas(train_set)).map(tokenize_function, batched=True)\n",
    "val_set_enc = (Dataset.from_pandas(val_set)).map(tokenize_function, batched=True)\n",
    "test_set_enc = (Dataset.from_pandas(test_set)).map(tokenize_function, batched=True)\n",
    "\n",
    "# Save to disk (recommended)\n",
    "train_set_enc.save_to_disk('lora_roberta_train_encodings.pt')\n",
    "val_set_enc.save_to_disk('lora_roberta_val_encodings.pt')\n",
    "test_set_enc.save_to_disk('lora_roberta_test_encodings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c34ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110460\n",
      "23670\n",
      "23670\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "train_set_enc = load_from_disk('lora_roberta_train_encodings.pt')\n",
    "print(len(train_set_enc))\n",
    "val_set_enc = load_from_disk('lora_roberta_val_encodings.pt')\n",
    "print(len(val_set_enc))\n",
    "test_set_enc = load_from_disk('lora_roberta_test_encodings.pt')\n",
    "print(len(test_set_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dataset_format(dataset):\n",
    "    dataset = dataset.rename_column('target', \"labels\")\n",
    "    dataset.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"labels\"]  # include \"labels\" column\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "train_dataset = set_dataset_format(train_set_enc)\n",
    "val_dataset = set_dataset_format(val_set_enc)\n",
    "test_dataset = set_dataset_format(test_set_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd4492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n",
      "Labels min, max: (tensor(0), tensor(14))\n",
      "Labels dtype: torch.int64\n",
      "batch.keys = dict_keys(['labels', 'input_ids', 'attention_mask']), \n",
      "tensor([   0,   74,   86,   69,   81,   73,   18,   88,   77,   81,   73,   30,\n",
      "         423,  322,   74, 2149, 1648,   74,   24,   72,  327,   71,  517,   72,\n",
      "        3352, 1806,   71, 2128,  279,  347,  407,  410,   31,  225,   77,   84,\n",
      "          18,   87,   86,   71,   67,   76,   83,   87,   88,   30,  383,  274,\n",
      "          28,   72,   29,  306,  883,  307,   27,   69,  753,   71,  874,   69,\n",
      "         308,  268,   20,  317,   28,  270,   20,  263,  884,   31,  225,   77,\n",
      "          84,   18,   72,   87,   88,   67,   76,   83,   87,   88,   30,  809,\n",
      "         263,   20,   71,   29,  797,  344,  268,   25,  795,   26,   74,  807,\n",
      "          72,   23,   73,   31,  739,   86,   84,   18,   72,   87,   88,   18,\n",
      "          84,   86,   83,   88,   83,   67,   77,   84,   90,   24,   30,  620,\n",
      "         618,  325,  621,  278,  321,   72,   21,  267,   26,  542,   20,  268,\n",
      "         581,  358,   24,   70,   23,   31,  739,   86,   84,   18,   83,   84,\n",
      "          71,   83,  309,   30,  381,  590,   69,  603,  602,  606,  599,   23,\n",
      "          70,  487,   69,  601,   73,   24,   31,  739,   86,   84,   18,   76,\n",
      "          91,   18,   87,   77,   94,   73,   30,  409,   72,  514,   69,   24,\n",
      "          70,  366,   71,   23,  326,  287,   72,  276,  357,   22,  339,   25,\n",
      "         317,  605,  317,  589,   31,  739,   86,   84,   18,   87,   86,   71,\n",
      "          18,   84,   86,   83,   88,   83,   67,   77,   84,   90,   24,   30,\n",
      "         713,  716,  433,   70,  711,  307,   26,   74,   22,   69,  295,   72,\n",
      "         715,   69,  673,   73,  685,   31,  225,   77,   71,   81,   84,   18,\n",
      "          71,   76,  292,   79,   87,   89,   81,   30,  381,  665,   74,  316,\n",
      "         268,   25,  648,   20,  282,   23,   74,  262,   70,   24,   73,  304,\n",
      "          71,  320,  395,  316,  317,   24,   31,  225,   77,   71,   81,   84,\n",
      "          18,   87,   73,   85,   67,   80,   73,   30,  341,  327,   72,  677,\n",
      "          69,   24,   70,  653,   72,  296,   70,  333,  314,  674,   73,   24,\n",
      "          73,  386,   31,  225,   77,   71,   81,   84,   18,   88,   86,   69,\n",
      "          82,   87,   81,   77,   88,   67,   88,   77,   81,   73,   87,   88,\n",
      "          69,   81,   84,   30,  466,  331,   27,   70,  287,  541,  277,  307,\n",
      "         534,   72,   29,   71,   25,  346,  532,   74,  536,  349,   23,  267,\n",
      "          31,  225,   77,   71,   81,   84,   18,   89,   82,   89,   87,  267,\n",
      "          30,  498,  294,   69,  322,  286,   29,   71,  303,   70,   27,   72,\n",
      "         565,  388,   26,  278,  334,  286,  376,   70,   31,  225,   76,   88,\n",
      "          88,   84,   18,   74,   77,   80,   73,   67,  767,   88,   69,   30,\n",
      "         341,  950,   70,   25,   71,  972,   69,  332,  318,  970,   71,   20,\n",
      "         357,  262,   69,  363,   71,  971,  346,  287,   31,  225,   76,   88,\n",
      "          88,   84,   18,   71,   83,   82,   88,   73,   82,   88,   67,   80,\n",
      "          73,   82,   75,   88,   76,   30,  645,   69,   20,  305,   28,   69,\n",
      "         515,   74,   22,  278,  642,  292,   23,   71,   24,  279,  315,  306,\n",
      "         644,   72,   27,   31,  225,   76,   88,   88,   84,   18,   86,   73,\n",
      "          85,   89,   73,   87,   88,   18,   89,   86,   77,   18,   85,   89,\n",
      "          73,   86,   93,   30,  374,   22,  274,  347,  274,  344,  438,  707,\n",
      "         708,   20,   70,   28,   69,  399,  292,   25,   73,  362,  286,   31,\n",
      "         225,   76,   88,   88,   84,   18,   86,    2])\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN=512\n",
    "BATCH_SIZE=32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0\n",
    "\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0\n",
    "\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0\n",
    "\n",
    ")\n",
    "\n",
    "test_data = next(iter(train_loader))\n",
    "\n",
    "print(test_data['input_ids'].shape)\n",
    "\n",
    "labels = train_dataset[\"labels\"]\n",
    "print(f\"Labels min, max: {labels.min(), labels.max()}\")\n",
    "print(f\"Labels dtype: {labels.dtype}\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(f\"batch.keys = {batch.keys()}, \")\n",
    "    break\n",
    "\n",
    "print(train_dataset['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68959c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chkpt(model,version):\n",
    "  return model.load_state_dict(torch.load(f\"./saved_model/securityRoBERTa{version}.0.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e6aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.epoch_train_losses = []\n",
    "        self.epoch_train_accuracies = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is None:\n",
    "            return\n",
    "        \n",
    "        if \"loss\" in logs:\n",
    "            self.epoch_train_losses.append(logs[\"loss\"])\n",
    "\n",
    "        if \"eval_accuracy\" in logs:\n",
    "            self.epoch_train_accuracies.append(logs[\"eval_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87687bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.history = defaultdict(list)\n",
    "        self.best_val_acc = 0.0\n",
    "\n",
    "    def evaluate_and_save(self, epoch, model_version):\n",
    "        # Evaluate on validation set\n",
    "        metrics = self.evaluate()\n",
    "        print(metrics.keys())\n",
    "        val_acc = metrics[\"eval_accuracy\"]\n",
    "        val_loss = metrics[\"eval_loss\"]\n",
    "        self.history[\"val_acc\"].append(val_acc)\n",
    "        self.history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = val_acc\n",
    "            self.save_model(f\"./saved_model/{model_version}{epoch+1}.0.pt\")\n",
    "            print(f\"Saved best model at epoch {epoch+1} with val_acc={val_acc:.4f}\")\n",
    "\n",
    "    def log_training_epoch(self, train_loss, train_acc):\n",
    "        self.history[\"train_loss\"].append(train_loss)\n",
    "        self.history[\"train_acc\"].append(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9e33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_qdora = torch.optim.AdamW(model.parameters(),lr=1e-5)\n",
    "total_steps = len(train_loader)*EPOCHS\n",
    "\n",
    "scheduler_qdora = get_linear_schedule_with_warmup(\n",
    "    optimizer_qdora,\n",
    "    num_warmup_steps= 0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b27086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# output dir \n",
    "model_version = \"securityRoBERTa_BaseQDoRA_\"\n",
    "model_dir = f\"{model_version}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    run_name=model_version,\n",
    "    output_dir=model_dir,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=1,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    logging_dir=f\"{model_dir}/logs\",\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    dataloader_num_workers=4,  # Adjust based on your CPU capabilities\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing to save memory\n",
    "    report_to=\"none\",  # Disable reporting to avoid unnecessary overhead\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e86fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1,072,143\n",
      "Total parameters: 68,101,357\n",
      "Percentage of trainable params: 1.57%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, confusion_matrix\n",
    "# The parameters after appling QDoRA\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"macro_f1\": macro_f1}\n",
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61746313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Train for one epoch (Trainer handles batches internally)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m trainer_qdora\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     25\u001b[0m trainer_qdora\u001b[38;5;241m.\u001b[39mlog_training_epoch(metrics_callback\u001b[38;5;241m.\u001b[39mepoch_train_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], metrics_callback\u001b[38;5;241m.\u001b[39mepoch_train_accuracies[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Evaluate and potentially save best model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\transformers\\trainer.py:2233\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2231\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2234\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2235\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2236\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2237\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2238\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\transformers\\trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2546\u001b[0m )\n\u001b[0;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2554\u001b[0m ):\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\transformers\\trainer.py:3740\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3739\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3740\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch)\n\u001b[0;32m   3742\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3745\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3746\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\transformers\\trainer.py:3807\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3805\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3806\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3807\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\peft\\peft_model.py:1559\u001b[0m, in \u001b[0;36mPeftModelForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[0;32m   1558\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[1;32m-> 1559\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[0;32m   1560\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1561\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1562\u001b[0m             inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1563\u001b[0m             labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1564\u001b[0m             output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1565\u001b[0m             output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1566\u001b[0m             return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1567\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1568\u001b[0m         )\n\u001b[0;32m   1570\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:193\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m, in \u001b[0;36mRobertaForSequenceClassificationWithCustomHead.forward\u001b[1;34m(self, input_ids, attention_mask, labels, **kwargs)\u001b[0m\n\u001b[0;32m     14\u001b[0m accepted_keys \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m }\n\u001b[0;32m     19\u001b[0m filtered_kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m accepted_keys}\n\u001b[1;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\n\u001b[0;32m     22\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m     23\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m     24\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfiltered_kwargs\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     28\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Move custom layer to correct device if needed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1214\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1202\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta(\n\u001b[0;32m   1203\u001b[0m     input_ids,\n\u001b[0;32m   1204\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1211\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1212\u001b[0m )\n\u001b[0;32m   1213\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1214\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n\u001b[0;32m   1216\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;66;03m# move labels to correct device to enable model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\peft\\utils\\other.py:399\u001b[0m, in \u001b[0;36mAuxiliaryTrainingWrapper.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_wrapped_disabled(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m adapter_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_wrapped(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mixed_batch_forward(x, \u001b[38;5;241m*\u001b[39margs, adapter_names\u001b[38;5;241m=\u001b[39madapter_names, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\peft\\utils\\other.py:465\u001b[0m, in \u001b[0;36mModulesToSaveWrapper._forward_wrapped\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_save[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters[\u001b[38;5;241m0\u001b[39m]](x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1455\u001b[0m, in \u001b[0;36mRobertaClassificationHead.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m   1453\u001b[0m x \u001b[38;5;241m=\u001b[39m features[:, \u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# take <s> token (equiv. to [CLS])\u001b[39;00m\n\u001b[0;32m   1454\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m-> 1455\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(x)\n\u001b[0;32m   1456\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(x)\n\u001b[0;32m   1457\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:480\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 480\u001b[0m     fix_4bit_weight_quant_state_from_module(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# weights are cast automatically as Int8Params, but the bias has to be cast manually\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype:\n",
      "File \u001b[1;32mc:\\Users\\oah1s\\anaconda3\\envs\\test_env\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:372\u001b[0m, in \u001b[0;36mfix_4bit_weight_quant_state_from_module\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m    366\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP4 quantization state not initialized. Please call .cuda() or .to(device) on the LinearFP4 layer first.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    368\u001b[0m     )\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# the quant state got lost when the parameter got converted. This happens for example for fsdp\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# since we registered the module, we can recover the state here\u001b[39;00m\n\u001b[1;32m--> 372\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m module\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module\u001b[38;5;241m.\u001b[39mweight, Params4bit):\n\u001b[0;32m    374\u001b[0m     module\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Params4bit(module\u001b[38;5;241m.\u001b[39mweight, quant_storage\u001b[38;5;241m=\u001b[39mmodule\u001b[38;5;241m.\u001b[39mquant_storage, bnb_quantized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "metrics_callback = MetricsCallback()\n",
    "\n",
    "# New trainer\n",
    "trainer_qdora = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=custom_tokenizer,\n",
    "    callbacks=[metrics_callback]\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    \n",
    "    # Train for one epoch (Trainer handles batches internally)\n",
    "    trainer_qdora.train()\n",
    "\n",
    "    trainer_qdora.log_training_epoch(metrics_callback.epoch_train_losses[-1], metrics_callback.epoch_train_accuracies[-1])\n",
    "\n",
    "    # Evaluate and potentially save best model\n",
    "    trainer_qdora.evaluate_and_save(\n",
    "        epoch,\n",
    "        model_version\n",
    "    )\n",
    "\n",
    "history_roberta_base_qdora = trainer_qdora.history\n",
    "\n",
    "# Save full history and training time\n",
    "end_time = time.time()\n",
    "history_roberta_base_qdora[\"training_time\"].append(end_time - start_time)\n",
    "\n",
    "torch.save(history_roberta_base_qdora, \"./saved_model/history_roberta_base_qdora.pt\")\n",
    "print(\"Training complete and history saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6381f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "history_roberta_base_qdora = torch.load(\"./saved_model/history_roberta_base_qdora.pt\", weights_only=False)\n",
    "\n",
    "# Still a defaultdict with tensors\n",
    "print(type(history_roberta_base_qdora))  # defaultdict\n",
    "print(type(history_roberta_base_qdora['train_acc'][0]))  # torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33331f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "epochs = range(1, EPOCHS+1)\n",
    "train_losses = history_roberta_base_qdora['train_loss']\n",
    "print(train_losses)\n",
    "val_losses = history_roberta_base_qdora['val_loss']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.lineplot(x=epochs, y=train_losses, label='Training Loss',marker=\"o\")\n",
    "sns.lineplot(x=epochs, y=val_losses, label='Validation Loss',marker=\"o\")\n",
    "\n",
    "plt.title('RoBERTa Base QDoRA Training and Validation Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('./roberta-base-qdora-training-validation-losses.png',dpi=780)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1febc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, EPOCHS+1)\n",
    "train_accuracies = history_roberta_base_qdora['train_acc']\n",
    "val_accuracies = history_roberta_base_qdora['val_acc']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.lineplot(x=epochs, y=train_accuracies, label='Training accuracy',marker='o')\n",
    "sns.lineplot(x=epochs, y=val_accuracies, label='Validation accuracy',marker='o')\n",
    "\n",
    "plt.title('RoBERTa Base QDoRA Training and Validation Accuracies')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('./roberta-base-qdora-training-validation-accuracies.png',dpi=780)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d803d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import PredictionOutput\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "def get_predictions(trainer, test_dataset):\n",
    "    # Runs prediction\n",
    "    output: PredictionOutput = trainer.predict(test_dataset)\n",
    "\n",
    "    # Get raw logits and labels\n",
    "    logits = torch.tensor(output.predictions)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "    labels = torch.tensor(output.label_ids)\n",
    "\n",
    "    return preds, probs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3accecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, probs, labels = get_predictions(trainer_qdora, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4510d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(preds,\"./saved_model/roberta_base_qdora_predictions.pt\")\n",
    "torch.save(probs,\"./saved_model/roberta_base_qdora_predictions_probs.pt\")\n",
    "torch.save(labels,\"./saved_model/roberta_base_qdora_real_values.pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee6873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Backdoor', 'DDoS_HTTP', 'DDoS_ICMP', 'DDoS_TCP', 'DDoS_UDP',\n",
    "       'Fingerprinting', 'MITM', 'Normal', 'Password', 'Port_Scanning',\n",
    "       'Ransomware', 'SQL_injection', 'Uploading', 'Vulnerability_scanner',\n",
    "       'XSS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a0e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in preds:\n",
    "  s.add(elt.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6825de",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_considered_classes = [TARGET_LIST[i] for i in s]\n",
    "actual_considered_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Attack_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e02f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='d')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"RoBERTa Base QDoRA Confusion Matrix\")\n",
    "    plt.ylabel('Real threats')\n",
    "    plt.xlabel('Predicted threats')\n",
    "    plt.savefig('./roberta-base-qdora-confusion-matrix.png',dpi=780)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d11d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels,preds,target_names=TARGET_LIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab68a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(labels,preds)\n",
    "df_cm = pd.DataFrame(cm,index=TARGET_LIST,columns=TARGET_LIST)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0881076",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = labels.cpu().numpy()\n",
    "y_score = preds.cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
